---
title: "Assignment 7"
author: "JUFENG YANG"
date: "2022-11-16"
output:
  pdf_document: 
    latex_engine: xelatex
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# ***Load packages***

```{R}
library(tidyverse)
```


# ***1. Maximum likelihood estimates***

## ***1.1 Maximum likelihood estimates for Red tailed hawks***

In this question we will fit a Gaussian model to a Red-Tailed hawk data set. First load the Hawks data set as
follows:

```{R}
library(Stat2Data)
data("Hawks")
#Hawks
```


### **1.1 (Q.1)**

```{R}
RedTailedDf <- Hawks %>% filter(Species == "RT") %>% select(c("Weight", "Tail", "Wing"))

```

### **1.1 (Q.2)**

We model the tail follow the N distribution. And the The maximum likelihood estimates for $\mu_0$ is given by $\mu_{MLE} =\frac{1}{n}\sum_{1}^{n}X_i$ and the maximum likelihood estimate for $\sigma_0^2$ is given by $\sigma^2_{MLE} =\frac{1}{n}\sum_{0}^{n}(Xi − \mu_MLE)^2.$ 

```{R}
mu_MLE <- RedTailedDf $ Tail %>% mean()
Tails <- RedTailedDf $ Tail
sigma_2_MLE <- RedTailedDf $ Tail %>% var() * ((length(Tails)) - 1) / (length(Tails))

mu_MLE
sigma_2_MLE
```


### **1.1 (Q.3)**
Next generate a plot which compares the probability density function for your fitted Gaussian model for the
tail length of the Red-Tailed hawks with a kernel density plot.

```{R}
x = Hawks$Tail
ker_density <- data.frame(x = x, ker = dnorm(x, mean = mu_MLE, sd = sqrt(sigma_2_MLE)))
density_comparise <- ggplot() + theme_bw() +
  geom_density(data = RedTailedDf, aes(x = Tail, colour = "Kerner density")) +
  geom_line(data = ker_density, aes(x = x, y = ker, colour = "MLE density")) +
  scale_colour_manual(name = "Density", 
                      values = c("Kerner density" = "blue", "MLE density" = "red") )
density_comparise
```


## ***1.2 Unbiased estimation of the population variance***

### **1.2 (Q.1)**
```{R}
Trails <- 1000
sample_size <- seq(5, 100, by = 5)

get_V <- function(sample, flag){
  mn = mean(sample)
  if(flag == "V_U"){
    variance = sum(map_dbl(.x = sample, .f = ~(.x - mn)^2)) / (length(sample) - 1)
  }else if(flag == "V_MLE"){
    variance = sum(map_dbl(.x = sample, .f = ~(.x - mn)^2)) / (length(sample))
  }
  
  #variance = var(sample) * length(sample) / (length(sample)-1)
  return(variance)
}

MLE_U_Compares_df <- crossing(Trails = seq(0,Trails), Sample_size = sample_size) %>%
  mutate(Sample = map(.x = Sample_size, .f = ~rnorm(.x, mean = 1, sd = 3))) %>% 
  #mutate(V_MLE = map_dbl(.x = Sample, .f = ~get_V(.x, flag = "V_MLE"))) %>% 
  #mutate(V_U = map_dbl(.x = Sample, .f = ~get_V(.x, flag = "V_U"))) %>%  
  mutate(V_U = map_dbl(.x = Sample, .f = var)) %>% 
  mutate(V_MLE = map2_dbl(.x = Sample, .y = Sample_size, .f = ~(var(.x) * (.y - 1) / .y)))

MLE_U_Compares_df

mean_MLE_U_Compares_df <- MLE_U_Compares_df %>% 
  group_by(Sample_size) %>% 
  summarise(mean_of_V_MLE = mean(V_MLE), mean_V_U = mean(V_U)) 

#ggplot() + geom_line(data = MLE_U_Compares_df, aes(x = sample_size, y = mean_of_V_MLE, color = "V_MLE"))
plot_MLE_U <- ggplot() + 
  geom_line(data = mean_MLE_U_Compares_df, 
            aes(x = sample_size, y = mean_of_V_MLE, color = "Mean_of_V_MLE")) +
  geom_line(data = mean_MLE_U_Compares_df, 
            aes(x = sample_size, y = mean_V_U, color = "Mean_of_V_U"))
plot_MLE_U

```

### **1.2 (Q.2)**

```{R}
mean_MLE_U_Compares_df <- MLE_U_Compares_df %>% 
  group_by(Sample_size) %>% 
  summarise(mean_of_V_MLE = mean(V_MLE), mean_V_U = mean(V_U)) 
mean_MLE_U_Compares_df
```
From the content of data frame, we can know the mean_V_U always around 9, so the V_U can unbiased estimator for $\sigma_0$


## ***1.3 Maximum likelihood estimation with the Poisson distribution***

### **1.3 (Q.1)**

for a sample $X_1. X_2,...., X_n$, the likelihood function $l:(0, \infty) \rightarrow (0, \infty)$is given by:

$$
\begin{aligned}
l(\lambda) = e^{-n\lambda} \cdot \lambda^{n\overline{X}} \cdot \bigg(\prod_{i=1}^{n}\frac{1}{X_i!}\bigg)
\end{aligned}
$$

The derivative of the log-likelihood: $\frac{\partial}{\partial \lambda} = \log l(\lambda)$

$$
\begin{aligned}
\frac{\partial}{\partial \lambda} \log l(\lambda) &= \frac{\partial}{\partial \lambda} \log \bigg[e^{-n\lambda} \cdot \lambda^{n\overline{X}} \cdot \bigg(\prod_{i=1}^{n}\frac{1}{X_i!}\bigg) \bigg] \\[2ex]
&= \frac{\partial}{\partial \lambda} \bigg [-n\lambda \ + \ n\overline{X} \log \lambda + \log \bigg(\prod_{i=1}^{n}\frac{1}{X_i!}\bigg) \bigg] \\[2ex]
&= -n \ + \ n\overline{X} \cdot \frac{1}{\lambda} \\[2ex]
&= n \cdot (\frac{\overline{X}}{\lambda} - 1)
\end{aligned}
$$

### **1.3 (Q.2)**

From last question, we get $\frac{\partial}{\partial \lambda} \log l(\lambda) = n \cdot (\frac{\overline{X}}{\lambda} - 1)$
When $\frac{\partial}{\partial \lambda} \log l(\lambda) = 0$, we can make $\log l(\lambda)$ reaches its maximum. So let $\frac{\overline{X}}{\lambda} = 1$, so $\lambda = \overline{X}$, Hence, the maximum likelihood estimate for the true parameter $\lambda^0$ is $\hat{\lambda}_{MLE} = \overline{X}$.

### **1.3 (Q.3)**

```{R}
lambda <- 0.5
sample_size <- 1000
r_V_pos <- rpois(sample_size, lambda = lambda)
mn_r_v <- mean(r_V_pos)
mn_r_v
```
Run the code above, we set $\lambda_0 = 0.5$, and generate a random variables followed by Poisson distribution, we compute the mean of those variables, and the mean is 0.501, so it prove $\overline{X} = \lambda_0$


### **1.3 (Q.4)**

```{R}
VonBortkiewicz <- read.csv("D:/Master in UoB/TB1 of UoB/SCEM/R project/Week8-Lab/VonBortkiewicz.csv")
fatalities <- VonBortkiewicz $ fatalities
lambda <- mean(fatalities)
density_poisson <- dpois(fatalities, lambda = lambda)
density_prob <- data.frame(fatalities, prob = density_poisson) %>% unique()
density_prob
# density_poisson
```

Run code above, we can know the fatalities is 0, the prob = 0.496585304, the fatalities = 1, the prob = 0.347609713, so when fatalities = 0, the probability is 0.4966.


## ***1.4 Maximum likelihood estimation for the exponential distribution***

### **1.4 (Q.1)**
Recall from our last assignment that given a positive real number $\lambda > 0$, an exponential random variable X
with parameter $\lambda$ is a continuous random variable with density $p_{\lambda} : R \rightarrow (0, \infty)$ define by
$$
p_{\lambda}(x)
\begin{cases}
0 \qquad &if \ \ x<0 \\[4ex]
\lambda e^{-\lambda x} \qquad &if \ \ x \geq 0
\end{cases}
$$
We can get the maximum likelihood function is $l(\lambda) = \prod_{i=1}^{i=n}p_{\lambda}(x \geq 0)$
So as bellow shows:
$$
\begin{aligned}
l(\lambda) &= \prod_{i=1}^{i=n}p_{\lambda}(x \geq 0) \\[2ex]
\text{Using log in two sides:}\\[2ex]
\log l(\lambda) &= \log \prod_{i=1}^{i=n}p_{\lambda}(x \geq 0) \\[2ex]
&= \log \prod_{i=1}^{i=n}\lambda e^{-\lambda x} \\[2ex]
\text{Using derivate in two sides:} \\[2ex]
\frac{\partial}{\partial \lambda} \log l(\lambda) &= \frac{\partial}{\partial \lambda} \log \prod_{i=1}^{i=n}\lambda e^{-\lambda x} \\[2ex]
&= \frac{\partial}{\partial \lambda} \bigg(\log \lambda^n + \log e^{-\lambda \cdot \sum_{i = 1}^{n} x_i} \bigg) \\[2ex]
&= \frac{\partial}{\partial \lambda} \bigg(\log \lambda^n + -\lambda \cdot \sum_{i = 1}^{n} x_i \bigg) \\[2ex]
&= \frac{n}{\lambda} -\lambda \cdot \sum_{i = 1}^{n} x_i \\[2ex]
&= \frac{n}{\lambda} - n\overline{x} \\[2ex]
\text{Let equation above equal to 0:} \\[2ex]
0 &= \frac{n}{\lambda} - n\overline{x}  \\[2ex]
&\Rightarrow \lambda = \frac{1}{\overline{X}}
\end{aligned}
$$

### **1.4 (Q.2)**

```{R}
CustomerPurchase <- read.csv("D:/Master in UoB/TB1 of UoB/SCEM/R project/Week8-Lab/CustomerPurchase.csv")
CustomerPurchase %>% head(30)
Time <- CustomerPurchase $ Time
Time %>% head(30)
time_diffs <- array(data = NA, dim = length(Time))
for(i in seq(1, length(Time) - 1, by = 1)){
  time_diffs[i] <- Time[i+1] - Time[i]
}
time_diffs %>% head(30)
CustomerPurchase <- CustomerPurchase %>% mutate(time_diffs)
CustomerPurchase %>% head(30)
#lambda <- mean(fatalities)
#density_poisson <- dpois(fatalities, lambda = lambda)
#density_prob <- data.frame(fatalities, prob = density_poisson) %>% unique()
#density_prob

```

### **1.4 (Q.3)**

Model the sequence of differences in purchase times X1, · · · , Xn as independent and identically distributed
exponential random variables. Compute the maximum likelihood estimate of the rate parameter $\lambda_{MLE}$.

```{R}
lambda_MLE <- 1 / (mean(time_diffs[1:length(time_diffs)-1]))
lambda_MLE
```


### **1.4 (Q.4)**

Use your fitted exponential model to give an estimate of the probability of an arrival time in excess of one
minute. You may wish to make use of the pexp() function.

```{R}
pexp(q = 60, rate = lambda_MLE)

```
Run the above code, the probability more than 60s is 0.7002107


# ***2. Confidence intervals***

## ***2.1 Student’s t-confidence intervals***

### **2.1 (Q.1)**

Sample mean do not change the width of confident interval, it just let the confident interval move to left or right.
if sample standard become bigger, the confident interval will become wider.
if sample size become bigger, the width of confident interval will become bigger.


### **2.1 (Q.2)**

```{R}
weight <- RedTailedDf $ Weight
weight <- weight[!is.na(weight)]

alpha <- 0.01
sample_size <- length(weight) # Weight is a given vector
sample_mean <- mean(weight)
sample_sd <- sd(weight)
t <- qt(1-alpha/2,df=sample_size-1)

# confidence interval
confidence_interval_l <- sample_mean-t*sample_sd/sqrt(sample_size)
confidence_interval_u <- sample_mean+t*sample_sd/sqrt(sample_size)
confidence_interval <- c(confidence_interval_l,confidence_interval_u)
confidence_interval

```

### **2.1 (Q.3)**

```{R}
ggplot() +  geom_density(data = RedTailedDf, aes(x = Weight, colour = "weight"))
ggplot(data = RedTailedDf, aes(sample = Weight)) + stat_qq() + stat_qq_line()
#+geom_line(data = ker_den, aes(x = weight, y = kern, colour = "weight")) +

```

## ***2.2 Investigating coverage for Student’s t intervals***

```{R}
student_t_confidence_interval <- function(sample, confidence_level){
  
sample <- sample[!is.na(sample)] # remove any missing values
n <- length(sample) # compute sample size
mu_est <- mean(sample) # compute sample mean
sig_est <- sd(sample) # compute sample sd
alpha = 1 - confidence_level # alpha from gamma
t <- qt(1 - alpha / 2, df = n-1) # get student t quantile
l = mu_est - (t / sqrt(n)) * sig_est # lower
u = mu_est + ( t / sqrt(n)) * sig_est # upper
return(c(l,u))

}
```


### **2.2 (Q.1)**

```{R}
num_trials <- 100000
sample_size <- 30
mu_0 <- 1
sigma_0 <- 3
alpha <- seq(0.01, 0.1, by = 0.01)
set.seed(0) # set random seed for reproducibility
single_alpha_coverage_simulation_df <- function(num_trials,
                                                sample_size,
                                                mu_0, sigma_0,
                                                alpha){

  midian_df <- data.frame(trial=seq(num_trials)) %>%
  # generate random Gaussian samples:
  mutate(sample=map(.x=trial, .f = ~rnorm(n = sample_size,
                                          mean = mu_0,
                                          sd = sigma_0))) %>%
  # generate confidence intervals:
  mutate(ci_interval = map(.x = sample,
                           .f = ~student_t_confidence_interval(.x, 1 - alpha)))%>%
  # check if interval covers mu_0:
  mutate(cover = map_lgl(.x = ci_interval,
                         .f = ~((min(.x) <= mu_0) & (max(.x) >= mu_0))))%>%
  # compute interval length:
  mutate(ci_length = map_dbl(.x = ci_interval,
                             .f = ~(max(.x) - min(.x))))
  # estimate of coverage probability:
  covers <- midian_df $ cover
  prob <- covers %>% mean()
  return(prob)
}

compares_alpha <- data.frame(confident_level = 1 - alpha) %>%
  mutate(prob_confident = map_dbl(.x = alpha,
                              .f = ~single_alpha_coverage_simulation_df(num_trials = 100000,
                                                   sample_size = 30,
                                                   mu_0 = 1,
                                                   sigma_0 = 3,
                                                   alpha = .x)))

compares_alpha

# prob_confodent <- array(data = NA, dim =  length(alpha))
# i <- 1
# 
# for(alpha_i in alpha){
#    probs <- single_alpha_coverage_simulation_df(num_trials = 100000, sample_size = 30, mu_0 = 1, sigma_0 = 3, alpha = alpha_i)
#    prob_confodent[i] <- probs
#    i = i + 1
# }
# 
# 
# compares_alpha <- data.frame(confident_level = 1 - alpha, prob_confodent)

```

It is easy to know the $P\{ L_\alpha(X_1,\dots, X_n) \leq \mu_0 \leq R_\alpha(X_1, \dots, X_n)\}$ has the same trend varies with 1 - $\alpha$


### **2.2 (Q.2)**

```{R}
num_trials <- 100000
sample_size <- 30
mu_0 <- 1
sigma_0 <- 3
alpha <- seq(0.01, 0.1, by = 0.01)
set.seed(0) # set random seed for reproducibility
single_alpha_coverage_simulation_df <- function(num_trials,
                                                sample_size,
                                                mu_0, sigma_0,
                                                alpha){

  midian_df <- data.frame(trial=seq(num_trials)) %>%
  # generate random Gaussian samples:
  mutate(sample=map(.x=trial, .f = ~rnorm(n = sample_size,
                                          mean = mu_0,
                                          sd = sigma_0))) %>%
  # generate confidence intervals:
  mutate(ci_interval = map(.x = sample,
                           .f = ~student_t_confidence_interval(.x, 1 - alpha)))%>%
  # check if interval covers mu_0:
  mutate(cover = map_lgl(.x = ci_interval,
                         .f = ~((min(.x) <= mu_0) & (max(.x) >= mu_0))))%>%
  # compute interval length:
  mutate(ci_length = map_dbl(.x = ci_interval, .f = ~(max(.x) - min(.x))))
  # estimate of coverage probability:
  ci_lengths <- midian_df $ ci_length
  ave_len <- ci_lengths %>% mean()
  return(ave_len)
}

ave_ci_length <- data.frame(confident_level = 1 - alpha) %>%
  mutate(ave_ci_length = map_dbl(.x = alpha,
                              .f = ~single_alpha_coverage_simulation_df(num_trials = 100000,
                                                   sample_size = 30,
                                                   mu_0 = 1,
                                                   sigma_0 = 3,
                                                   alpha = .x)))

ave_ci_length
```


# ***3. One sample hypothesis testing***

## ***3.1 One sample t-test on penguins data***

```{R}

library(palmerpenguins)
data("penguins")

bill_Adelie <- penguins %>% filter(species == "Adelie") %>% pull(bill_length_mm)
bill_Adelie <- bill_Adelie[!is.na(bill_Adelie)]

t.test(bill_Adelie, mu = 40)

```

The p-value is 1.114e-07 is less than 0.01, so we reject this hypothesis and conclude $\mu_0 \neq 40$


## ***3.2 Implementing a one-sample t-test***

Create a t test:

```{R}
my_t_test <- function(x, mu){
  T <- (mean(x) - mu) / (sd(x) / sqrt(length(x)))
  p_value <- 2 * (1 - pt(abs(T), df = length(x) - 1))
  #mn_p <- mean(p_value)
  return(p_value)
}

my_t_test(x = bill_Adelie, mu = 40)

```
The function above is the t-test function I created, when the $\mu = 40$ is tested, the p-value is 1.14322e-07. The p-value is same as the in-built function t-test. 


